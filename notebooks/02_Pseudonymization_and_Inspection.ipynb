{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e06e143-008e-4252-98f1-22ff51868e4f",
   "metadata": {},
   "source": [
    "##### ═══════════════════════════════════════════════════════════\n",
    "##### PSEUDONYMIZATION & INSPECTION NOTEBOOK\n",
    "##### Creates privacy-safe versions of marketing and streaming data using David Lynch character names and deterministic hashing. Also streaming numbers perturbation and change to project numbers and campaign ids. Checks how many artists are present in both streaming and marketing dataset, or only in one of them.\n",
    "##### ═══════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68982d1d-29a3-4a03-81ff-f6db68deb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 0. Imports\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3ea374-fbac-4915-bf2c-7c8f623198d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Marketing: 13,835 rows\n",
      "  Marketing unique artists - Canonical names file - cleaned: 79\n",
      "  Marketing unique products - Canonical names file- cleaned: 126\n",
      "  Streaming: 614,005 rows\n",
      "  Streaming unique artists - Canonical names file- cleaned: 72\n",
      "  Streaming unique products - Canonical names file- cleaned: 910\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 1. LOAD DATA\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "m_df = pd.read_csv('original_marketing_canonical_names.csv', encoding='ISO-8859-1')\n",
    "s_df = pd.read_csv('original_streaming_canonical_names.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(f\"  Marketing: {len(m_df):,} rows\")\n",
    "print(f\"  Marketing unique artists - Canonical names file - cleaned: {m_df['canonical_artist'].nunique()}\")\n",
    "print(f\"  Marketing unique products - Canonical names file- cleaned: {m_df['canonical_product'].nunique()}\")\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "print(f\"  Streaming: {len(s_df):,} rows\")\n",
    "print(f\"  Streaming unique artists - Canonical names file- cleaned: {s_df['canonical_artist'].nunique()}\")\n",
    "print(f\"  Streaming unique products - Canonical names file- cleaned: {s_df['canonical_product'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3585b778-8964-462e-9c0c-40514c4d338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pseudonym pools loaded\n",
      "  Artists available: 113\n",
      "  Songs available: 1019\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 2. LOAD PSEUDONYM POOLS\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def load_pool(path, column):\n",
    "    \"\"\"Load pseudonym pool from CSV file\"\"\"\n",
    "    # Try utf-8-sig first (for files with BOM), fall back to cp1252\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding='cp1252')\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in {path}\")\n",
    "    vals = df[column].dropna().astype(str).tolist()\n",
    "    if not vals:\n",
    "        raise ValueError(f\"No values found in column '{column}' of {path}\")\n",
    "    return vals\n",
    "    \n",
    "# Load David Lynch character names for artists and songs\n",
    "artist_pool = load_pool('pseudonym_artists.csv', 'pseudonym_artist')\n",
    "song_pool = load_pool('pseudonym_songs.csv', 'pseudonym_song')\n",
    "\n",
    "print(f\"✓ Pseudonym pools loaded\")\n",
    "print(f\"  Artists available: {len(artist_pool)}\")\n",
    "print(f\"  Songs available: {len(song_pool)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791480c1-23c4-4e21-a394-8d87cd3e42a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 3. HELPER FUNCTIONS FOR DETERMINISTIC MAPPING\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def stable_index(key: str, modulo: int, salt: str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Generate deterministic index from key using SHA-256 hashing.\n",
    "    Same key always produces same index.\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256((salt + str(key)).encode(\"utf-8\")).hexdigest()\n",
    "    return int(h, 16) % modulo\n",
    "\n",
    "def assign_unique(values, pool, salt):\n",
    "    \"\"\"\n",
    "    Deterministically assign unique pseudonyms to values.\n",
    "    Uses hashing + linear probing to avoid collisions.\n",
    "    \n",
    "    Args:\n",
    "        values: list of real values to pseudonymize\n",
    "        pool: list of available pseudonyms\n",
    "        salt: string to ensure different mappings for different purposes\n",
    "    \n",
    "    Returns:\n",
    "        dict mapping real value → pseudonym\n",
    "    \"\"\"\n",
    "   \n",
    "    uniques = list(dict.fromkeys(map(str, values)))\n",
    "    \n",
    "    if len(uniques) > len(pool):\n",
    "        raise ValueError(f\"Need {len(uniques)} pseudonyms; pool has {len(pool)}.\")\n",
    "    \n",
    "    n = len(pool)\n",
    "    used_pseudonyms = set()  # Track pseudonyms, not indices\n",
    "    mapping = {}\n",
    "    \n",
    "    for val in uniques:\n",
    "        idx = stable_index(val, n, salt)\n",
    "        start = idx\n",
    "        \n",
    "        while pool[idx] in used_pseudonyms:  # Check if this pseudonym is taken\n",
    "            idx = (idx + 1) % n\n",
    "            if idx == start:\n",
    "                raise RuntimeError(\"Exhausted pool\")\n",
    "        \n",
    "        used_pseudonyms.add(pool[idx])\n",
    "        mapping[val] = pool[idx]\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def assign_numeric_ids(values, digits=12, salt=\"\"):\n",
    "    \"\"\"\n",
    "    Generate deterministic numeric IDs for project_no and campaign_id.\n",
    "    \n",
    "    Args:\n",
    "        values: list of real IDs\n",
    "        digits: length of generated numeric ID\n",
    "        salt: string to ensure different mappings for projects vs campaigns\n",
    "    \n",
    "    Returns:\n",
    "        dict mapping real ID → pseudonymized numeric ID\n",
    "    \"\"\"\n",
    "    uniques = sorted(set(map(str, values)))\n",
    "    mapping = {}\n",
    "    used = set()\n",
    "    modulo = 10 ** digits\n",
    "    \n",
    "    for val in uniques:\n",
    "        # Generate deterministic number from hash\n",
    "        h_int = int(hashlib.sha256((salt + val).encode(\"utf-8\")).hexdigest(), 16)\n",
    "        code = h_int % modulo\n",
    "        start = code\n",
    "        \n",
    "        # Linear probing to avoid collisions\n",
    "        while code in used:\n",
    "            code = (code + 1) % modulo\n",
    "            if code == start:\n",
    "                raise RuntimeError(\"ID space exhausted\")\n",
    "        \n",
    "        used.add(code)\n",
    "        mapping[val] = str(code).zfill(digits)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def normalize_key(series):\n",
    "    \"\"\"\n",
    "    Normalize keys for consistent matching.\n",
    "    Removes trailing .0 from numbers: '12345.0' → '12345'\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "    return s\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab36b685-e566-4186-b83e-e3e597c55583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ IDs normalized\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 4. NORMALIZE IDs IN BOTH FILES\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "# Ensure consistent string format for IDs (remove trailing .0)\n",
    "m_df['project_no'] = normalize_key(m_df['project_no'])\n",
    "s_df['project_no'] = normalize_key(s_df['project_no'])\n",
    "\n",
    "if 'campaign_id' in m_df.columns:\n",
    "    m_df['campaign_id'] = normalize_key(m_df['campaign_id'])\n",
    "\n",
    "print(\"✓ IDs normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb828eb-36a6-4d16-a867-dc6506ffe2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total unique projects: 82\n",
      "  Total unique campaigns: 111\n",
      "  Total unique canonical artists: 82\n",
      "\n",
      "✓ All unique values collected from both files\n",
      "  Total unique canonical products: 962\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 5. COLLECT ALL UNIQUE VALUES FROM BOTH FILES\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# This ensures consistent pseudonyms across both files!\n",
    "\n",
    "# Collect all project numbers from both files\n",
    "all_projects = set(m_df['project_no']) | set(s_df['project_no'])\n",
    "print(f\"  Total unique projects: {len(all_projects)}\")\n",
    "\n",
    "# Collect all campaign IDs (only in marketing)\n",
    "all_campaigns = set()\n",
    "if 'campaign_id' in m_df.columns:\n",
    "    all_campaigns = set(m_df['campaign_id'])\n",
    "print(f\"  Total unique campaigns: {len(all_campaigns)}\")\n",
    "\n",
    "# Collect all canonical artists from both files\n",
    "all_artists = set(m_df['canonical_artist'].dropna()) | set(s_df['canonical_artist'].dropna())\n",
    "print(f\"  Total unique canonical artists: {len(all_artists)}\")\n",
    "\n",
    "# Collect all canonical products from both files\n",
    "all_products = set(m_df['canonical_product'].dropna()) | set(s_df['canonical_product'].dropna())\n",
    "\n",
    "print(f\"\\n✓ All unique values collected from both files\")\n",
    "print(f\"  Total unique canonical products: {len(all_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d3c026-8c19-4ddc-a63c-9667d7cf3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project mapping created: 82 projects\n",
      "✓ Campaign mapping created: 111 campaigns\n",
      "✓ Artist mapping created: 82 artists\n",
      "✓ Product mapping created: 962 products\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 6. BUILD PSEUDONYM MAPPINGS\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Each mapping is deterministic: same input always gives same output\n",
    "\n",
    "SALT = \"lynch_salt_v6\"  # Salt ensures different mappings for different purposes\n",
    "\n",
    "# Map project_no → numeric pseudonym (e.g., \"12345\" → \"847362910485\")\n",
    "project_map = assign_numeric_ids(all_projects, digits=12, salt=SALT + \"_project\")\n",
    "print(f\"✓ Project mapping created: {len(project_map)} projects\")\n",
    "\n",
    "# Map campaign_id → numeric pseudonym\n",
    "campaign_map = {}\n",
    "if all_campaigns:\n",
    "    campaign_map = assign_numeric_ids(all_campaigns, digits=12, salt=SALT + \"_campaign\")\n",
    "    print(f\"✓ Campaign mapping created: {len(campaign_map)} campaigns\")\n",
    "\n",
    "# Map canonical_artist → David Lynch character (e.g., \"Rihanna\" → \"Audrey_Horne\")\n",
    "# Uses project_no implicitly since canonical_artist is tied to project\n",
    "artist_map = assign_unique(sorted(all_artists), artist_pool, SALT + \"_artist\")\n",
    "print(f\"✓ Artist mapping created: {len(artist_map)} artists\")\n",
    "\n",
    "# Map product_key → David Lynch location/object (e.g., \"12345_Umbrella\" → \"Black_Lodge\")\n",
    "product_map = assign_unique(sorted(all_products), song_pool, SALT + \"_song\")\n",
    "print(f\"✓ Product mapping created: {len(product_map)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32f9609-739c-4a38-a1ff-711e668ce8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stream noise applied (±10%)\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 7. ADD STREAM NOISE (±10% per project)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Adds controlled noise to stream counts to protect privacy\n",
    "# Each project gets consistent noise multiplier (0.9 to 1.1)\n",
    "\n",
    "rng = random.Random(101)  # Fixed seed for reproducibility\n",
    "\n",
    "# Create noise multiplier for each project (before pseudonymization)\n",
    "project_noise = {\n",
    "    proj: rng.uniform(0.9, 1.1)\n",
    "    for proj in all_projects\n",
    "}\n",
    "\n",
    "# Apply noise to streaming data\n",
    "if 'streams' in s_df.columns:\n",
    "    s_df['streams'] = s_df.apply(\n",
    "        lambda row: int(row['streams'] * project_noise[row['project_no']])\n",
    "        if pd.notnull(row['streams']) else row['streams'],\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"✓ Stream noise applied (±10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9944f15-e1ec-4999-8e18-c314be334003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pseudonymizing marketing data...\n",
      "✓ Marketing data pseudonymized\n",
      "  Rows: 13,835\n",
      "  Columns: 46\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 8. APPLY PSEUDONYMIZATION TO MARKETING FILE\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "print(\"\\nPseudonymizing marketing data...\")\n",
    "\n",
    "# Pseudonymize project_no\n",
    "m_df['project_no'] = m_df['project_no'].map(project_map)\n",
    "# Convert to Int64 to preserve precision when saving\n",
    "m_df['project_no'] = m_df['project_no'].astype('Int64')\n",
    "\n",
    "# Pseudonymize campaign_id\n",
    "if 'campaign_id' in m_df.columns and campaign_map:\n",
    "    m_df['campaign_id'] = m_df['campaign_id'].map(campaign_map)\n",
    "\n",
    "# Pseudonymize artist_name using canonical_artist mapping\n",
    "m_df['artist_name'] = m_df['canonical_artist'].map(artist_map)\n",
    "# Pseudonymize canonical_artist\n",
    "m_df['canonical_artist'] = m_df['canonical_artist'].map(artist_map)\n",
    "\n",
    "# Pseudonymize product columns - map directly from canonical_product\n",
    "m_df['canonical_product'] = m_df['canonical_product'].map(product_map)\n",
    "m_df['product'] = m_df['product'].map(product_map)\n",
    "\n",
    "print(\"✓ Marketing data pseudonymized\")\n",
    "print(f\"  Rows: {len(m_df):,}\")\n",
    "print(f\"  Columns: {len(m_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4395e91-409e-4edd-bb33-7d70b5273db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pseudonymizing streaming data...\n",
      "NaN values after mapping: 0\n",
      "Unique canonical_product: 910\n",
      "✓ Streaming data pseudonymized\n",
      "  Rows: 614,005\n",
      "  Columns: 9\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 9. APPLY PSEUDONYMIZATION TO STREAMING FILE\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "print(\"\\nPseudonymizing streaming data...\")\n",
    "\n",
    "# Pseudonymize project_no\n",
    "s_df['project_no'] = s_df['project_no'].map(project_map)\n",
    "# Convert to Int64 to preserve precision when saving:\n",
    "s_df['project_no'] = s_df['project_no'].astype('Int64')\n",
    "\n",
    "# Pseudonymize artist_name using canonical_artist mapping\n",
    "s_df['artist_name'] = s_df['canonical_artist'].map(artist_map)\n",
    "# Pseudonymize canonical_artist\n",
    "s_df['canonical_artist'] = s_df['canonical_artist'].map(artist_map)\n",
    "\n",
    "# Pseudonymize product columns - map directly from canonical_product\n",
    "s_df['canonical_product'] = s_df['canonical_product'].map(product_map)\n",
    "s_df['product_name'] = s_df['product_name'].map(product_map)\n",
    "\n",
    "# Check mapping\n",
    "print(f\"NaN values after mapping: {s_df['canonical_product'].isna().sum()}\")\n",
    "print(f\"Unique canonical_product: {s_df['canonical_product'].nunique()}\")\n",
    "\n",
    "print(\"✓ Streaming data pseudonymized\")\n",
    "print(f\"  Rows: {len(s_df):,}\")\n",
    "print(f\"  Columns: {len(s_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8ef373-ea35-491e-a356-e301c59e02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PSEUDONYMIZATION VERIFICATION\n",
      "============================================================\n",
      "\n",
      "MARKETING FILE:\n",
      "  Unique projects: 80\n",
      "  Unique artists: 79\n",
      "  Unique products: 126\n",
      "  Unique campaigns: 111\n",
      "\n",
      "STREAMING FILE:\n",
      "  Unique projects: 72\n",
      "  Unique artists: 72\n",
      "  Unique products: 910\n",
      "\n",
      "Sample pseudonymized data (marketing):\n",
      "     project_no canonical_artist  canonical_product\n",
      "0  432191283317    Senorita_Dido          True_Face\n",
      "1  791586853774         Log_Lady  Got_a_Light_Again\n",
      "2  663102993334      Gordon_Cole       Rita_Appears\n",
      "\n",
      "Sample pseudonymized data (streaming):\n",
      "     project_no canonical_artist canonical_product\n",
      "0  318733585791    Nadine_Hurley      Red_Bathrobe\n",
      "1  616610447432  Annie_Blackburn           No_Hope\n",
      "2  963210974654     Tom_Beaumont      Inner_Vision\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# 10. VERIFY PSEUDONYMIZATION\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PSEUDONYMIZATION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nMARKETING FILE:\")\n",
    "print(f\"  Unique projects: {m_df['project_no'].nunique()}\")\n",
    "print(f\"  Unique artists: {m_df['canonical_artist'].nunique()}\")\n",
    "print(f\"  Unique products: {m_df['canonical_product'].nunique()}\")\n",
    "if 'campaign_id' in m_df.columns:\n",
    "    print(f\"  Unique campaigns: {m_df['campaign_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nSTREAMING FILE:\")\n",
    "print(f\"  Unique projects: {s_df['project_no'].nunique()}\")\n",
    "print(f\"  Unique artists: {s_df['canonical_artist'].nunique()}\")\n",
    "print(f\"  Unique products: {s_df['canonical_product'].nunique()}\")\n",
    "\n",
    "print(\"\\nSample pseudonymized data (marketing):\")\n",
    "print(m_df[['project_no','canonical_artist', 'canonical_product']].head(3))\n",
    "\n",
    "print(\"\\nSample pseudonymized data (streaming):\")\n",
    "print(s_df[['project_no', 'canonical_artist', 'canonical_product']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb7e1a-6a4f-4711-a0c6-5d4ae23e8d31",
   "metadata": {},
   "source": [
    "We have one artist, Sailor Ripley (and the original artist in the original file), who is an exception to the \"one artist, one project_no\" rule. They have two project numbers, one for an album, one for a track (not pertaining to that album). That's how we have 79 artists and 80 project numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a246963-42fe-4c1a-a4ed-9ebad1f5c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.to_csv('streaming_pseudonymized_new.csv')\n",
    "m_df.to_csv('marketing_pseudonymized_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36c702b8-53ae-4d83-9301-5c61b2371c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 61.80 MB\n",
      "Compressed: 12.01 MB\n",
      "Savings: 80.6%\n"
     ]
    }
   ],
   "source": [
    "# Check size\n",
    "size_mb = os.path.getsize('streaming_pseudonymized_new.csv') / (1024**2)\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Compress it\n",
    "with open('streaming_pseudonymized_new.csv', 'rb') as f_in:\n",
    "    with gzip.open('streaming_pseudonymized_new.csv.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Check compressed size\n",
    "compressed_mb = os.path.getsize('streaming_pseudonymized_new.csv.gz') / (1024**2)\n",
    "print(f\"Compressed: {compressed_mb:.2f} MB\")\n",
    "print(f\"Savings: {(1 - compressed_mb/size_mb)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a1fa73-c3f8-4f35-be6d-7267c8f33bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artists in both files (artists who had both ad campaigns and streams): 69\n",
      "Example overlapping artists: ['Major_Briggs', 'Shelly_Johnson', 'Norma_Jennings', 'Rebecca_Del_Rio', 'The_Fireman']\n",
      "\n",
      "Major_Briggs:\n",
      "  Marketing project_no: 106206807592\n",
      "  Streaming project_no: 106206807592\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# MATCHED ARTISTS\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Find artists that appear in BOTH files\n",
    "marketing_artists = set(m_df['canonical_artist'].unique())\n",
    "streaming_artists = set(s_df['canonical_artist'].unique())\n",
    "overlap = marketing_artists.intersection(streaming_artists)\n",
    "\n",
    "print(f\"\\nArtists in both files (artists who had both ad campaigns and streams): {len(overlap)}\")\n",
    "print(f\"Example overlapping artists: {list(overlap)[:5]}\")\n",
    "\n",
    "# Pick one overlapping artist and show their project_no in both files\n",
    "if overlap:\n",
    "    test_artist = list(overlap)[0]\n",
    "    m_project = m_df[m_df['canonical_artist'] == test_artist]['project_no'].iloc[0]\n",
    "    s_project = s_df[s_df['canonical_artist'] == test_artist]['project_no'].iloc[0]\n",
    "    print(f\"\\n{test_artist}:\")\n",
    "    print(f\"  Marketing project_no: {m_project}\")\n",
    "    print(f\"  Streaming project_no: {s_project}\")\n",
    "    print(f\"  Match: {m_project == s_project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "483454b4-8605-4223-947e-95247b8c882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UNMATCHED ARTISTS\n",
      "============================================================\n",
      "\n",
      "ARTISTS: Present in only one file\n",
      "------------------------------------------------------------\n",
      "   Marketing only - Signed artists & Compilations that had campaigns but had no streams(10):\n",
      "     - Coco\n",
      "     - Diane_Selwyn\n",
      "     - Lady_Jessica\n",
      "     - Marietta_Fortune\n",
      "     - Monica_Bellucci_Herself\n",
      "     - Mr_Roque\n",
      "     - Mrs_Tremonds_Grandson\n",
      "     - Nikki_Grace\n",
      "     - Pete_Martell\n",
      "     - The_Man_from_Another_Place\n",
      "\n",
      "   Streaming only - Signed artists that had no campaigns but still had streams (3):\n",
      "     - Gersten_Hayward\n",
      "     - Harold_Smith\n",
      "     - Lady_Margot_Fenring\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# UNMATCHED ARTISTS\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UNMATCHED ARTISTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# 1. Artists only in one file\n",
    "print(\"\\nARTISTS: Present in only one file\")\n",
    "print(\"-\" * 60)\n",
    "marketing_artists = set(m_df['canonical_artist'].unique())\n",
    "streaming_artists = set(s_df['canonical_artist'].unique())\n",
    "both_artists = marketing_artists & streaming_artists\n",
    "\n",
    "print(f\"   Marketing only - Signed artists & Compilations that had campaigns but had no streams({len(marketing_artists - streaming_artists)}):\")\n",
    "for a in sorted(marketing_artists - streaming_artists):\n",
    "    print(f\"     - {a}\")\n",
    "\n",
    "print(f\"\\n   Streaming only - Signed artists that had no campaigns but still had streams ({len(streaming_artists - marketing_artists)}):\")\n",
    "for a in sorted(streaming_artists - marketing_artists):\n",
    "    print(f\"     - {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c836f9ee-bbbd-4e26-85b6-1996bc35df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ARTISTS IN BOTH FILES WITH NO OVERLAPPING PRODUCTS\n",
      "------------------------------------------------------------\n",
      "Artists in both datasets with NO shared products: 12\n",
      "→ 17.4% of artists present in both files with no overlapping products\n"
     ]
    }
   ],
   "source": [
    "# Check artists present in both files but with no overlapping products\n",
    "print(\"\\n ARTISTS IN BOTH FILES WITH NO OVERLAPPING PRODUCTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "artists_m = set(m_df['canonical_artist'].unique())\n",
    "artists_s = set(s_df['canonical_artist'].unique())\n",
    "\n",
    "both_artists = artists_m & artists_s\n",
    "\n",
    "artists_both_no_overlap = []\n",
    "\n",
    "for artist in both_artists:\n",
    "    m_products = set(\n",
    "        m_df[m_df['canonical_artist'] == artist]['canonical_product']\n",
    "    )\n",
    "    s_products = set(\n",
    "        s_df[s_df['canonical_artist'] == artist]['canonical_product']\n",
    "    )\n",
    "    \n",
    "    if len(m_products & s_products) == 0:\n",
    "        artists_both_no_overlap.append(artist)\n",
    "        \n",
    "num_no_overlap = len(artists_both_no_overlap)\n",
    "pct_no_overlap = num_no_overlap / len(both_artists) * 100\n",
    "\n",
    "print(f\"Artists in both datasets with NO shared products: {num_no_overlap}\")\n",
    "print(f\"→ {pct_no_overlap:.1f}% of artists present in both files with no overlapping products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ffe2c-5c25-403d-9fc9-0b8fba6ea450",
   "metadata": {},
   "source": [
    "This doesn't look ideal, having 1/6 of the advertised products not correlate with any streams. But we remember that some of our products were albums or playlists or giveaways for fans, i.e. if they generated streams, they don't have a direct match in our streaming dataset, where records of streams are kept at track level. We need to inspect the share of each product_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a75c74-976c-44bb-9510-4137f58edf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_number_type</th>\n",
       "      <th>product_type_on_spotify</th>\n",
       "      <th>unique_products</th>\n",
       "      <th>pct_of_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URL</td>\n",
       "      <td>no_spotify_link</td>\n",
       "      <td>6</td>\n",
       "      <td>42.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Track URI</td>\n",
       "      <td>track</td>\n",
       "      <td>4</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Album URI</td>\n",
       "      <td>album</td>\n",
       "      <td>3</td>\n",
       "      <td>21.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Playlist URI</td>\n",
       "      <td>playlist</td>\n",
       "      <td>2</td>\n",
       "      <td>14.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_number_type product_type_on_spotify  unique_products  \\\n",
       "3                 URL         no_spotify_link                6   \n",
       "2           Track URI                   track                4   \n",
       "0           Album URI                   album                3   \n",
       "1        Playlist URI                playlist                2   \n",
       "\n",
       "   pct_of_products  \n",
       "3        42.857143  \n",
       "2        28.571429  \n",
       "0        21.428571  \n",
       "1        14.285714  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_m_df = m_df[\n",
    "    m_df['canonical_artist'].isin(artists_both_no_overlap)\n",
    "]\n",
    "\n",
    "product_type_summary = (\n",
    "    problem_m_df\n",
    "    .groupby(['product_number_type', 'product_type_on_spotify'])\n",
    "    ['canonical_product']\n",
    "    .nunique()\n",
    "    .reset_index(name='unique_products')\n",
    ")\n",
    "\n",
    "product_type_summary.sort_values('unique_products', ascending=False)\n",
    "\n",
    "total_problem_products = problem_m_df['canonical_product'].nunique()\n",
    "\n",
    "product_type_summary['pct_of_products'] = (\n",
    "    product_type_summary['unique_products'] / total_problem_products * 100\n",
    ")\n",
    "\n",
    "product_type_summary.sort_values('pct_of_products', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81605e-2f4b-4433-badd-db270bbf0443",
   "metadata": {},
   "source": [
    "This looks much better: Only 4 advertised tracks (songs) didn't have streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aed8948a-1471-461f-903a-dfca2a24ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        canonical_artist canonical_product    project_no\n",
      "15         Count_Fenring      Red_Curtains  511340909446\n",
      "162      Annie_Blackburn  Together_at_last  616610447432\n",
      "716           The_Cowboy         Horseshoe  633147923560\n",
      "1462  Detective_Williams        Purple_Sea  351022636206\n"
     ]
    }
   ],
   "source": [
    "# Find out what the problematic tracks are\n",
    "\n",
    "problematic_tracks_df = (\n",
    "    problem_m_df[\n",
    "        (problem_m_df['product_number_type'] == 'Track URI') &\n",
    "        (problem_m_df['product_type_on_spotify'] == 'track')\n",
    "    ]\n",
    "    [['canonical_artist', 'canonical_product', 'project_no']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "print(problematic_tracks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c75fd4-ab65-4f48-a4a1-75807116f908",
   "metadata": {},
   "source": [
    "# Horseshoe is Think about it by 2Hot2Play, das hat Streams.... Vielleicht capitalization war ein issue???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e893ca-6d6c-4788-b95a-2d6bfe0707c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. EXPORTING MAPPINGS WITH ORIGINAL NAMES\n",
      "------------------------------------------------------------\n",
      "   ✓ Saved artist_product_mapping_with_originals.csv (82 artists)\n",
      "   ✓ Saved product_artist_mapping_with_originals.csv (962 products)\n",
      "\n",
      "   Files saved with both pseudonyms and original names\n"
     ]
    }
   ],
   "source": [
    "# Export mappings WITH original names for verification\n",
    "print(\"\\n7. EXPORTING MAPPINGS WITH ORIGINAL NAMES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create reverse mappings from the pseudonymization dictionaries\n",
    "artist_reverse = {v: k for k, v in artist_map.items()}\n",
    "product_reverse = {v: k for k, v in product_map.items()}\n",
    "\n",
    "# Create artist mapping with original names\n",
    "artist_mapping_export = []\n",
    "for artist in sorted(set(m_df['canonical_artist'].unique()) | set(s_df['canonical_artist'].unique())):\n",
    "    in_marketing = artist in m_df['canonical_artist'].values\n",
    "    in_streaming = artist in s_df['canonical_artist'].values\n",
    "    \n",
    "    m_products = sorted(m_df[m_df['canonical_artist'] == artist]['canonical_product'].unique()) if in_marketing else []\n",
    "    s_products = sorted(s_df[s_df['canonical_artist'] == artist]['canonical_product'].unique()) if in_streaming else []\n",
    "    overlap_products = sorted(set(m_products) & set(s_products))\n",
    "    \n",
    "    # Get original names\n",
    "    original_artist = artist_reverse.get(artist, artist)\n",
    "    original_m_products = [product_reverse.get(p, p) for p in m_products]\n",
    "    original_s_products = [product_reverse.get(p, p) for p in s_products]\n",
    "    original_overlap = [product_reverse.get(p, p) for p in overlap_products]\n",
    "    \n",
    "    artist_mapping_export.append({\n",
    "        'pseudonym_artist': artist,\n",
    "        'original_artist': original_artist,\n",
    "        'in_marketing': in_marketing,\n",
    "        'in_streaming': in_streaming,\n",
    "        'marketing_product_count': len(m_products),\n",
    "        'streaming_product_count': len(s_products),\n",
    "        'overlap_product_count': len(overlap_products),\n",
    "        'marketing_products_pseudo': ', '.join(m_products) if m_products else '',\n",
    "        'marketing_products_original': ', '.join(original_m_products) if original_m_products else '',\n",
    "        'streaming_products_pseudo': ', '.join(s_products) if s_products else '',\n",
    "        'streaming_products_original': ', '.join(original_s_products) if original_s_products else '',\n",
    "        'overlap_products_pseudo': ', '.join(overlap_products) if overlap_products else '',\n",
    "        'overlap_products_original': ', '.join(original_overlap) if original_overlap else ''\n",
    "    })\n",
    "\n",
    "artist_mapping_df = pd.DataFrame(artist_mapping_export)\n",
    "artist_mapping_df.to_csv('artist_product_mapping_with_originals.csv', index=False)\n",
    "print(f\"   ✓ Saved artist_product_mapping_with_originals.csv ({len(artist_mapping_df)} artists)\")\n",
    "\n",
    "# Create product mapping with original names\n",
    "product_mapping_export = []\n",
    "for product in sorted(set(m_df['canonical_product'].unique()) | set(s_df['canonical_product'].unique())):\n",
    "    in_marketing = product in m_df['canonical_product'].values\n",
    "    in_streaming = product in s_df['canonical_product'].values\n",
    "    \n",
    "    m_artists = sorted(m_df[m_df['canonical_product'] == product]['canonical_artist'].unique()) if in_marketing else []\n",
    "    s_artists = sorted(s_df[s_df['canonical_product'] == product]['canonical_artist'].unique()) if in_streaming else []\n",
    "    \n",
    "    # Get original names\n",
    "    original_product = product_reverse.get(product, product)\n",
    "    original_m_artists = [artist_reverse.get(a, a) for a in m_artists]\n",
    "    original_s_artists = [artist_reverse.get(a, a) for a in s_artists]\n",
    "    \n",
    "    product_mapping_export.append({\n",
    "        'pseudonym_product': product,\n",
    "        'original_product': original_product,\n",
    "        'in_marketing': in_marketing,\n",
    "        'in_streaming': in_streaming,\n",
    "        'marketing_artists_pseudo': ', '.join(m_artists) if m_artists else '',\n",
    "        'marketing_artists_original': ', '.join(original_m_artists) if original_m_artists else '',\n",
    "        'streaming_artists_pseudo': ', '.join(s_artists) if s_artists else '',\n",
    "        'streaming_artists_original': ', '.join(original_s_artists) if original_s_artists else ''\n",
    "    })\n",
    "\n",
    "product_mapping_df = pd.DataFrame(product_mapping_export)\n",
    "product_mapping_df.to_csv('product_artist_mapping_with_originals.csv', index=False)\n",
    "print(f\"   ✓ Saved product_artist_mapping_with_originals.csv ({len(product_mapping_df)} products)\")\n",
    "\n",
    "print(f\"\\n   Files saved with both pseudonyms and original names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60436be8-a873-49e2-8cad-0df7a126cfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM25",
   "language": "python",
   "name": "cb22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
